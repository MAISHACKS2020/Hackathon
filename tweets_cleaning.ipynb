{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pro = pd.read_csv(\"/home/jahnic/Git/Hackathon/pro_trump2.csv\",index_col=0)\n",
    "data_anti = pd.read_csv(\"/home/jahnic/Git/Hackathon/anti_trump.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "@Jeromep81422970  And I thought  would blabla @melania\n #stillgray And I thought #Trump would blabla \n"
    }
   ],
   "source": [
    "def remove_mentions(tweet):\n",
    "    \"\"\"Removes mentions of the form @NAME123\"\"\"\n",
    "    x = re.sub('@[a-zA-Z0-9]+', \"\", tweet)\n",
    "    return x\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    x = re.sub('#[a-zA-Z0-9]+', \"\", tweet)\n",
    "    return x\n",
    "\n",
    "# test\n",
    "s = \"@Jeromep81422970 #stillgray And I thought #Trump would blabla @melania\"\n",
    "print(remove_hashtags(s))\n",
    "print(remove_mentions(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                tweets\n0    &quot;So after Biden wins, what do we do with the c...\n1    &quot;@GOPLeader @GoJackFlynn Please. \\nJust #Remov...\n2    &quot;@Jeromep81422970 @stillgray And I thought #Tr...\n3    &quot;Seeing a lot of #TrumpSupporters taking their...\n4    &quot;The bigger question is, why are hundreds of p...\n..                                                 ...\n841  &quot;Considering 1969 was the best year in US Hist...\n843  &quot;#MaskUpAmerica \\n\\n#TrumpSupporters are reall...\n844  &quot;EXACTLY why NO ONE could change my mind! #tru...\n845  &quot;This is the ‘woman’who mailed a ricin-laced e...\n846  &quot;@realDonaldTrump #RacistInChief #TrumpIsRacis...\n\n[824 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"So after Biden wins, what do we do with the c...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"@GOPLeader @GoJackFlynn Please. \\nJust #Remov...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"@Jeromep81422970 @stillgray And I thought #Tr...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"Seeing a lot of #TrumpSupporters taking their...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"The bigger question is, why are hundreds of p...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>841</th>\n      <td>\"Considering 1969 was the best year in US Hist...</td>\n    </tr>\n    <tr>\n      <th>843</th>\n      <td>\"#MaskUpAmerica \\n\\n#TrumpSupporters are reall...</td>\n    </tr>\n    <tr>\n      <th>844</th>\n      <td>\"EXACTLY why NO ONE could change my mind! #tru...</td>\n    </tr>\n    <tr>\n      <th>845</th>\n      <td>\"This is the ‘woman’who mailed a ricin-laced e...</td>\n    </tr>\n    <tr>\n      <th>846</th>\n      <td>\"@realDonaldTrump #RacistInChief #TrumpIsRacis...</td>\n    </tr>\n  </tbody>\n</table>\n<p>824 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "data_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                              tweets  label\n0  &quot;So after Biden wins, what do we do with the c...      1\n1  &quot;@GOPLeader @GoJackFlynn Please. \\nJust #Remov...      0\n2  &quot;@Jeromep81422970 @stillgray And I thought #Tr...      0\n3  &quot;Seeing a lot of #TrumpSupporters taking their...      0\n4  &quot;The bigger question is, why are hundreds of p...      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"So after Biden wins, what do we do with the c...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"@GOPLeader @GoJackFlynn Please. \\nJust #Remov...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"@Jeromep81422970 @stillgray And I thought #Tr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"Seeing a lot of #TrumpSupporters taking their...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"The bigger question is, why are hundreds of p...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "## add a fake label\n",
    "fakeLabel = np.random.randint(2,size=462)\n",
    "data[\"label\"] = fakeLabel\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /home/jahnic/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "# basic modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "# text preprocessing modules\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from tokenize import tokenize\n",
    "\n",
    "\n",
    "# models\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dowload stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "#download punkt for word_tokensize\n",
    "#nltk.download('punkt')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "finally\n"
    }
   ],
   "source": [
    "def remove_stopwords(input_data):\n",
    "    '''\n",
    "    input_data: a vector of messages\n",
    "    output: stopwords removed\n",
    "    '''\n",
    "    # required:\n",
    "    # import nltk\n",
    "    # dowload stopwords\n",
    "    # nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    output = []\n",
    "    for i, sentence in enumerate(input_data):\n",
    "        #words = word_tokenize(sentence)  #will not remove punctuations and numbers\n",
    "        sms = re.sub(\"[^a-zA-Z]\", \" \", sentence).lower()\n",
    "        words = word_tokenize(sms)\n",
    "        filtered_sentence = [w for w in words if not w in stop_words]\n",
    "        output.append(\" \".join(filtered_sentence).lower() )\n",
    "    return output\n",
    "\n",
    "#https://rustyonrampage.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "print(reduce_lengthening( \"finallllllly\" ) )\n",
    "\n",
    "def remove_stemmers(input_data):\n",
    "    '''\n",
    "    input: a vector of messages \n",
    "    output: remvove stemmers and punctuations\n",
    "        all lower letter \n",
    "        stemmer removed\n",
    "        punctuations removed\n",
    "    '''\n",
    "    ## require download punkt for word_tokensize\n",
    "    #nltk.download('punkt')\n",
    "    #stemmer = PorterStemmer()\n",
    "    \n",
    "    ## https://towardsdatascience.com/multi-class-text-classification-with-sklearn-and-nltk-in-python-a-software-engineering-use-case-779d4a28ba5\n",
    "    ## df_train[\"cleaned\"] = df_train['message'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "    output = []\n",
    "    for i, sentence in enumerate(input_data):\n",
    "        sms = re.sub(\"[^a-zA-Z]\", \" \", sentence) \n",
    "        words = word_tokenize(sms)\n",
    "        filtered_sentence = [stemmer.stem(reduce_lengthening(w)) for w in words]\n",
    "        output.append(\" \".join(filtered_sentence) )\n",
    "        \n",
    "    return output\n",
    "\n",
    "def messages_preprocess(input_data):      \n",
    "    return remove_stemmers(remove_stopwords(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "time used: 0.41307902336120605\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                              tweets  label  \\\n0  &quot;So after Biden wins, what do we do with the c...      1   \n1  &quot;@GOPLeader @GoJackFlynn Please. \\nJust #Remov...      0   \n2  &quot;@Jeromep81422970 @stillgray And I thought #Tr...      0   \n3  &quot;Seeing a lot of #TrumpSupporters taking their...      0   \n4  &quot;The bigger question is, why are hundreds of p...      0   \n\n                                             cleaned  \n0                                     biden win cult  \n1  goplead gojackflynn pleas njust removepelosi n...  \n2  jeromep stillgray thought trumpsupport bot tru...  \n3  see lot trumpsupport take maga hat last night ...  \n4             bigger question hundr peopl buse ralli  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>label</th>\n      <th>cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"So after Biden wins, what do we do with the c...</td>\n      <td>1</td>\n      <td>biden win cult</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"@GOPLeader @GoJackFlynn Please. \\nJust #Remov...</td>\n      <td>0</td>\n      <td>goplead gojackflynn pleas njust removepelosi n...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"@Jeromep81422970 @stillgray And I thought #Tr...</td>\n      <td>0</td>\n      <td>jeromep stillgray thought trumpsupport bot tru...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"Seeing a lot of #TrumpSupporters taking their...</td>\n      <td>0</td>\n      <td>see lot trumpsupport take maga hat last night ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"The bigger question is, why are hundreds of p...</td>\n      <td>0</td>\n      <td>bigger question hundr peopl buse ralli</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "#preprocess df_train\n",
    "start = time.time()\n",
    "\n",
    "data['cleaned'] = messages_preprocess(data[\"tweets\"])  #about 1 min\n",
    "\n",
    "end = time.time()\n",
    "print(\"time used:\",end - start)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "###################################################\nUncleaned bag of words size: (462, 2934)\n###################################################\ncleaned bag of words size: (462, 2612)\n###################################################\nUncleaned IT-IDF: (462, 2931)\n###################################################\nCleaned IT-IDF: (462, 2609)\n"
    }
   ],
   "source": [
    "# creating the feature matrix \n",
    "\n",
    "#Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=100000, min_df=1, max_df=0.7,stop_words=stopwords.words('english'))\n",
    "\n",
    "#with uncleaned messages\n",
    "count_vec = vectorizer.fit_transform(data['tweets'])\n",
    "count_vec = count_vec.toarray()\n",
    "print(\"###################################################\")\n",
    "print(\"Uncleaned bag of words size:\", count_vec.shape)\n",
    "\n",
    "#with cleaned messages\n",
    "count_vec_clean = vectorizer.fit_transform(data['cleaned'])\n",
    "count_vec_clean = count_vec_clean.toarray()\n",
    "print(\"###################################################\")\n",
    "print(\"cleaned bag of words size:\", count_vec_clean.shape)\n",
    "\n",
    "#Tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#with uncleaned messages\n",
    "tfidf_vec = TfidfVectorizer(max_features=100000, min_df=1, max_df=0.5,stop_words=stopwords.words('english'))\n",
    "tfidf_vec = tfidf_vec.fit_transform(data['tweets']).toarray()\n",
    "print(\"###################################################\")\n",
    "print(\"Uncleaned IT-IDF:\", tfidf_vec.shape)\n",
    "\n",
    "#with cleaned messages\n",
    "tfidf_vec_clean = TfidfVectorizer(max_features=100000, min_df=1, max_df=0.5,stop_words=stopwords.words('english'))\n",
    "tfidf_vec_clean = tfidf_vec_clean.fit_transform(data['cleaned']).toarray()\n",
    "print(\"###################################################\")\n",
    "print(\"Cleaned IT-IDF:\", tfidf_vec_clean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose preprocessed data as training data\n",
    "processed_list = [count_vec, count_vec_clean, tfidf_vec, tfidf_vec_clean]\n",
    "processed_data = processed_list[2]  # [count_vec, count_vec_clean, tfidf_vec, tfidf_vec_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                tweets  label  \\\n199                                      &quot;#WalkAway \\&quot;      1   \n248  &quot;The woman who was arrested and charged with a...      0   \n276                                      &quot;#WalkAway \\&quot;      1   \n289                                                &quot;\\&quot;      1   \n\n                                               cleaned  \n199                                           walkaway  \n248  woman arrest charg attempt murder drive crowd ...  \n276                                           walkaway  \n289                                                     ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>label</th>\n      <th>cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>199</th>\n      <td>\"#WalkAway \\\"</td>\n      <td>1</td>\n      <td>walkaway</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>\"The woman who was arrested and charged with a...</td>\n      <td>0</td>\n      <td>woman arrest charg attempt murder drive crowd ...</td>\n    </tr>\n    <tr>\n      <th>276</th>\n      <td>\"#WalkAway \\\"</td>\n      <td>1</td>\n      <td>walkaway</td>\n    </tr>\n    <tr>\n      <th>289</th>\n      <td>\"\\\"</td>\n      <td>1</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "data[data.cleaned.duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance of count_vec(0.48) is worse than tfidf (0.54)\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(processed_data,\n",
    "                                                                                 data['label'], \n",
    "                                                                                 data.index, \n",
    "                                                                                 test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "alpha =  0.1\n",
      "val acc =  0.5483870967741935\n",
      "train acc =  0.989159891598916\n",
      "########################################\n",
      "alpha =  0.2\n",
      "val acc =  0.5161290322580645\n",
      "train acc =  0.986449864498645\n",
      "########################################\n",
      "alpha =  0.30000000000000004\n",
      "val acc =  0.5161290322580645\n",
      "train acc =  0.981029810298103\n",
      "########################################\n",
      "alpha =  0.4\n",
      "val acc =  0.5161290322580645\n",
      "train acc =  0.981029810298103\n",
      "########################################\n",
      "alpha =  0.5\n",
      "val acc =  0.4946236559139785\n",
      "train acc =  0.981029810298103\n",
      "########################################\n",
      "alpha =  0.6\n",
      "val acc =  0.5053763440860215\n",
      "train acc =  0.981029810298103\n",
      "########################################\n",
      "alpha =  0.7000000000000001\n",
      "val acc =  0.5053763440860215\n",
      "train acc =  0.981029810298103\n",
      "########################################\n",
      "alpha =  0.8\n",
      "val acc =  0.5053763440860215\n",
      "train acc =  0.981029810298103\n",
      "########################################\n",
      "alpha =  0.9\n",
      "val acc =  0.5053763440860215\n",
      "train acc =  0.981029810298103\n",
      "########################################\n",
      "alpha =  1.0\n",
      "val acc =  0.4946236559139785\n",
      "train acc =  0.981029810298103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "test_acc_list = []\n",
    "train_acc_list = []\n",
    "alpha_list = [0.1 + x*0.1 for x in range(10)]\n",
    "for alpha in alpha_list:\n",
    "\n",
    "    model = MultinomialNB(alpha = alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(\"########################################\")\n",
    "    print(\"alpha = \", alpha)\n",
    "    \n",
    "    test_acc_list += [accuracy_score(y_test, y_pred)]\n",
    "    \n",
    "    print(\"val acc = \", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # performence on training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_acc_list += [accuracy_score(y_train_pred, y_train)]\n",
    "    print(\"train acc = \", accuracy_score(y_train_pred, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}