{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"pro_trump2_400.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"tweets\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"So after Biden wins, what do we do with the c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"@GOPLeader @GoJackFlynn Please. \\nJust #Remov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"@Jeromep81422970 @stillgray And I thought #Tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Seeing a lot of #TrumpSupporters taking their...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"The bigger question is, why are hundreds of p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>\"#TrumpSupporters you suck\\n#Republicans you s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>\"BIDEN’S WISCONSIN WELCOME PARTY: Just look at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>\"Name me one single way @JoeBiden can win over...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>\"Donald can't say there'll be a peaceful trans...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>\"@benshapiro Looking for the #TrumpSupporters ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets  label\n",
       "0    \"So after Biden wins, what do we do with the c...      1\n",
       "1    \"@GOPLeader @GoJackFlynn Please. \\nJust #Remov...      1\n",
       "2    \"@Jeromep81422970 @stillgray And I thought #Tr...      0\n",
       "3    \"Seeing a lot of #TrumpSupporters taking their...      0\n",
       "4    \"The bigger question is, why are hundreds of p...      0\n",
       "..                                                 ...    ...\n",
       "388  \"#TrumpSupporters you suck\\n#Republicans you s...      0\n",
       "389  \"BIDEN’S WISCONSIN WELCOME PARTY: Just look at...      1\n",
       "390  \"Name me one single way @JoeBiden can win over...      1\n",
       "391  \"Donald can't say there'll be a peaceful trans...      1\n",
       "397  \"@benshapiro Looking for the #TrumpSupporters ...      0\n",
       "\n",
       "[254 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 for anti-trump, 1 for pro-trump, 2 for unknown\n",
    "data = data[data.iloc[:,1]!=2]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of pro-trump tweets\n",
    "sum(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## add a fake label\n",
    "# fakeLabel = np.random.randint(2,size=462)\n",
    "# data[\"label\"] = fakeLabel\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/xiao/.local/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: joblib in /home/xiao/.local/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: click in /home/xiao/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /home/xiao/.local/lib/python3.8/site-packages (from nltk) (4.50.0)\n",
      "Requirement already satisfied: regex in /home/xiao/.local/lib/python3.8/site-packages (from nltk) (2020.9.27)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/xiao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# basic modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "# text preprocessing modules\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from tokenize import tokenize\n",
    "\n",
    "\n",
    "# models\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/xiao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/xiao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#dowload stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "#download punkt for word_tokensize\n",
    "nltk.download('punkt')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(input_data):\n",
    "    '''\n",
    "    input_data: a vector of messages\n",
    "    output: stopwords removed\n",
    "    '''\n",
    "    # required:\n",
    "    # import nltk\n",
    "    # dowload stopwords\n",
    "    # nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    output = []\n",
    "    for i, sentence in enumerate(input_data):\n",
    "        #words = word_tokenize(sentence)  #will not remove punctuations and numbers\n",
    "        sms = re.sub(\"[^a-zA-Z]\", \" \", sentence).lower()\n",
    "        words = word_tokenize(sms)\n",
    "        filtered_sentence = [w for w in words if not w in stop_words]\n",
    "        output.append(\" \".join(filtered_sentence).lower() )\n",
    "    return output\n",
    "\n",
    "#https://rustyonrampage.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "print(reduce_lengthening( \"finallllllly\" ) )\n",
    "\n",
    "def remove_stemmers(input_data):\n",
    "    '''\n",
    "    input: a vector of messages \n",
    "    output: remvove stemmers and punctuations\n",
    "        all lower letter \n",
    "        stemmer removed\n",
    "        punctuations removed\n",
    "    '''\n",
    "    ## require download punkt for word_tokensize\n",
    "    #nltk.download('punkt')\n",
    "    #stemmer = PorterStemmer()\n",
    "    \n",
    "    ## https://towardsdatascience.com/multi-class-text-classification-with-sklearn-and-nltk-in-python-a-software-engineering-use-case-779d4a28ba5\n",
    "    ## df_train[\"cleaned\"] = df_train['message'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "    output = []\n",
    "    for i, sentence in enumerate(input_data):\n",
    "        sms = re.sub(\"[^a-zA-Z]\", \" \", sentence) \n",
    "        words = word_tokenize(sms)\n",
    "        filtered_sentence = [stemmer.stem(reduce_lengthening(w)) for w in words]\n",
    "        output.append(\" \".join(filtered_sentence) )\n",
    "        \n",
    "    return output\n",
    "\n",
    "def messages_preprocess(input_data):      \n",
    "    return remove_stemmers(remove_stopwords(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used: 0.13688087463378906\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"So after Biden wins, what do we do with the c...</td>\n",
       "      <td>1</td>\n",
       "      <td>biden win cult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"@GOPLeader @GoJackFlynn Please. \\nJust #Remov...</td>\n",
       "      <td>1</td>\n",
       "      <td>goplead gojackflynn pleas njust removepelosi n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"@Jeromep81422970 @stillgray And I thought #Tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>jeromep stillgray thought trumpsupport bot tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Seeing a lot of #TrumpSupporters taking their...</td>\n",
       "      <td>0</td>\n",
       "      <td>see lot trumpsupport take maga hat last night ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"The bigger question is, why are hundreds of p...</td>\n",
       "      <td>0</td>\n",
       "      <td>bigger question hundr peopl buse ralli</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  label  \\\n",
       "0  \"So after Biden wins, what do we do with the c...      1   \n",
       "1  \"@GOPLeader @GoJackFlynn Please. \\nJust #Remov...      1   \n",
       "2  \"@Jeromep81422970 @stillgray And I thought #Tr...      0   \n",
       "3  \"Seeing a lot of #TrumpSupporters taking their...      0   \n",
       "4  \"The bigger question is, why are hundreds of p...      0   \n",
       "\n",
       "                                             cleaned  \n",
       "0                                     biden win cult  \n",
       "1  goplead gojackflynn pleas njust removepelosi n...  \n",
       "2  jeromep stillgray thought trumpsupport bot tru...  \n",
       "3  see lot trumpsupport take maga hat last night ...  \n",
       "4             bigger question hundr peopl buse ralli  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess df_train\n",
    "start = time.time()\n",
    "\n",
    "data['cleaned'] = messages_preprocess(data[\"tweets\"])  #about 1 min\n",
    "\n",
    "end = time.time()\n",
    "print(\"time used:\",end - start)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "Uncleaned bag of words size: (254, 1896)\n",
      "###################################################\n",
      "cleaned bag of words size: (254, 1689)\n",
      "###################################################\n",
      "Uncleaned IT-IDF: (254, 1896)\n",
      "###################################################\n",
      "Cleaned IT-IDF: (254, 1689)\n"
     ]
    }
   ],
   "source": [
    "# creating the feature matrix \n",
    "\n",
    "#Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=100000, min_df=1, max_df=0.7,stop_words=stopwords.words('english'))\n",
    "\n",
    "#with uncleaned messages\n",
    "count_vec = vectorizer.fit_transform(data['tweets'])\n",
    "count_vec = count_vec.toarray()\n",
    "print(\"###################################################\")\n",
    "print(\"Uncleaned bag of words size:\", count_vec.shape)\n",
    "\n",
    "#with cleaned messages\n",
    "count_vec_clean = vectorizer.fit_transform(data['cleaned'])\n",
    "count_vec_clean = count_vec_clean.toarray()\n",
    "print(\"###################################################\")\n",
    "print(\"cleaned bag of words size:\", count_vec_clean.shape)\n",
    "\n",
    "#Tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#with uncleaned messages\n",
    "tfidf_vec = TfidfVectorizer(max_features=100000, min_df=1, max_df=0.5,stop_words=stopwords.words('english'))\n",
    "tfidf_vec = tfidf_vec.fit_transform(data['tweets']).toarray()\n",
    "print(\"###################################################\")\n",
    "print(\"Uncleaned IT-IDF:\", tfidf_vec.shape)\n",
    "\n",
    "#with cleaned messages\n",
    "tfidf_vec_clean = TfidfVectorizer(max_features=100000, min_df=1, max_df=0.5,stop_words=stopwords.words('english'))\n",
    "tfidf_vec_clean = tfidf_vec_clean.fit_transform(data['cleaned']).toarray()\n",
    "print(\"###################################################\")\n",
    "print(\"Cleaned IT-IDF:\", tfidf_vec_clean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose preprocessed data as training data\n",
    "processed_list = [count_vec, count_vec_clean, tfidf_vec, tfidf_vec_clean]\n",
    "processed_data = processed_list[2]  # [count_vec, count_vec_clean, tfidf_vec, tfidf_vec_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance of count_vec(0.48) is worse than tfidf (0.54)\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(processed_data,\n",
    "                                                                                 data['label'], \n",
    "                                                                                 data.index, \n",
    "                                                                                 test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "alpha =  0.1\n",
      "val acc =  0.7058823529411765\n",
      "train acc =  0.9950738916256158\n",
      "########################################\n",
      "alpha =  0.2\n",
      "val acc =  0.6862745098039216\n",
      "train acc =  0.9950738916256158\n",
      "########################################\n",
      "alpha =  0.30000000000000004\n",
      "val acc =  0.6862745098039216\n",
      "train acc =  0.9950738916256158\n",
      "########################################\n",
      "alpha =  0.4\n",
      "val acc =  0.7058823529411765\n",
      "train acc =  0.9901477832512315\n",
      "########################################\n",
      "alpha =  0.5\n",
      "val acc =  0.7254901960784313\n",
      "train acc =  0.9901477832512315\n",
      "########################################\n",
      "alpha =  0.6\n",
      "val acc =  0.6862745098039216\n",
      "train acc =  0.9901477832512315\n",
      "########################################\n",
      "alpha =  0.7000000000000001\n",
      "val acc =  0.6862745098039216\n",
      "train acc =  0.9802955665024631\n",
      "########################################\n",
      "alpha =  0.8\n",
      "val acc =  0.7058823529411765\n",
      "train acc =  0.9802955665024631\n",
      "########################################\n",
      "alpha =  0.9\n",
      "val acc =  0.7058823529411765\n",
      "train acc =  0.9802955665024631\n",
      "########################################\n",
      "alpha =  1.0\n",
      "val acc =  0.6862745098039216\n",
      "train acc =  0.9802955665024631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "test_acc_list = []\n",
    "train_acc_list = []\n",
    "alpha_list = [0.1 + x*0.1 for x in range(10)]\n",
    "for alpha in alpha_list:\n",
    "\n",
    "    model = MultinomialNB(alpha = alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(\"########################################\")\n",
    "    print(\"alpha = \", alpha)\n",
    "    \n",
    "    test_acc_list += [accuracy_score(y_test, y_pred)]\n",
    "    \n",
    "    print(\"val acc = \", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # performence on training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_acc_list += [accuracy_score(y_train_pred, y_train)]\n",
    "    print(\"train acc = \", accuracy_score(y_train_pred, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
